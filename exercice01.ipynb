{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j3cB5nzrFZqM"
      },
      "source": [
        "# Initialisation de CUDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BJMIqMIupjl9"
      },
      "outputs": [],
      "source": [
        "!nvcc -V"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "_0WZ72H9WKYc"
      },
      "outputs": [],
      "source": [
        "!apt-get --purge remove cuda nvidia* libnvidia-*\n",
        "!dpkg -l | grep cuda- | awk '{print $2}' | xargs -n1 dpkg --purge\n",
        "!apt-get remove cuda-*\n",
        "!apt autoremove\n",
        "!apt-get update"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gAeionhAWlPp"
      },
      "outputs": [],
      "source": [
        "!wget https://developer.nvidia.com/compute/cuda/9.2/Prod/local_installers/cuda-repo-ubuntu1604-9-2-local_9.2.88-1_amd64 -O cuda-repo-ubuntu1604-9-2-local_9.2.88-1_amd64.deb\n",
        "!dpkg -i cuda-repo-ubuntu1604-9-2-local_9.2.88-1_amd64.deb\n",
        "!apt-key add /var/cuda-repo-9-2-local/7fa2af80.pub\n",
        "!apt-get update\n",
        "!apt-get install cuda-9.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lAuTDb_SpyIj"
      },
      "outputs": [],
      "source": [
        "!pip install git+git://github.com/andreinechaev/nvcc4jupyter.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mlg1rb6tqPMX"
      },
      "outputs": [],
      "source": [
        "%load_ext nvcc_plugin"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-gjIFJHE-kq"
      },
      "source": [
        "# TP2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jwl-4BEerRo5"
      },
      "outputs": [],
      "source": [
        "%%cu\n",
        "#ifndef __CUDACC__\n",
        "#define __CUDACC__\n",
        "#endif\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <math.h>\n",
        "#include <string.h>\n",
        "#include \"cuda_runtime.h\"\n",
        "#include \"device_launch_parameters.h\"\n",
        "#include \"utils.h\"\n",
        "\n",
        "\n",
        "#define A_WIDTH 1024\n",
        "#define A_HEIGHT 1024\n",
        "#define B_WIDTH 1024\n",
        "#define B_HEIGHT 1024\n",
        "#define C_WIDTH B_WIDTH\n",
        "#define C_HEIGHT A_HEIGHT\n",
        "\n",
        "#define BLOCK_SIZE 8\n",
        "#define NUM_SUBS (A_WIDTH / BLOCK_SIZE)\n",
        "\n",
        "__device__ float d_A[A_HEIGHT][A_WIDTH];\n",
        "__device__ float d_B[B_HEIGHT][B_WIDTH];\n",
        "__device__ float d_C[C_HEIGHT][C_WIDTH];\n",
        "\n",
        "float h_A[A_HEIGHT][A_WIDTH];\n",
        "float h_B[B_HEIGHT][B_WIDTH];\n",
        "float h_C[C_HEIGHT][C_WIDTH];\n",
        "float h_C_ref[C_HEIGHT][C_WIDTH];\n",
        "\n",
        "struct event_pair\n",
        "{\n",
        "  cudaEvent_t start;\n",
        "  cudaEvent_t end;\n",
        "};\n",
        "\n",
        "inline void start_timer(event_pair * p)\n",
        "{\n",
        "  cudaEventCreate(&p->start);\n",
        "  cudaEventCreate(&p->end);\n",
        "  cudaEventRecord(p->start, 0);\n",
        "}\n",
        "\n",
        "\n",
        "inline void stop_timer(event_pair * p, char * kernel_name)\n",
        "{\n",
        "  cudaEventRecord(p->end, 0);\n",
        "  cudaEventSynchronize(p->end);\n",
        "  \n",
        "  float elapsed_time;\n",
        "  cudaEventElapsedTime(&elapsed_time, p->start, p->end);\n",
        "  printf(\"%s took %.4f ms\\n\",kernel_name, elapsed_time);\n",
        "  cudaEventDestroy(p->start);\n",
        "  cudaEventDestroy(p->end);\n",
        "}\n",
        "\n",
        "void checkCUDAError(const char *msg);\n",
        "void matMulCPU(float A[A_HEIGHT][A_WIDTH], float B[B_HEIGHT][B_WIDTH], float C[C_HEIGHT][C_WIDTH]);\n",
        "int matMulValidate(float C[C_HEIGHT][C_WIDTH], float Cref[C_HEIGHT][C_WIDTH]);\n",
        "\n",
        "__global__ void matMulKernel()\n",
        "{\n",
        "    // Index des blocs et des threads\n",
        "\tint bx = blockIdx.x;\n",
        "\tint by = blockIdx.y;\n",
        "\tint tx = threadIdx.x;\n",
        "\tint ty = threadIdx.y;\n",
        "\t// 1.2.1 Indexation globale des threads\n",
        "\tint x = \n",
        "\tint y = \n",
        "    \n",
        "\n",
        "\tfloat Csub = 0;\n",
        "\t// On itere sur A_WIDTH (meme que B_HEIGHT) pour calculer le produit\n",
        "\tfor (int k = 0; k < A_WIDTH; k++){\n",
        "\t\t// 1.2.2 Multiplication matricielle entre une ligne de A et une colonne de B\n",
        "\t\tCsub += \n",
        "\t}\n",
        "\n",
        "\t// On stocke le resultat dans la matrice C\n",
        "\td_C[y][x] = Csub;\n",
        "}\n",
        "\n",
        "__global__ void matMulKernelSharedMemory()\n",
        "{\n",
        "    // Creation d'une sous-matrice de A et de B dans la memoire partagee.\n",
        "    __shared__ float As[BLOCK_SIZE][BLOCK_SIZE];\n",
        "\t__shared__ float Bs[BLOCK_SIZE][BLOCK_SIZE];\n",
        "    \n",
        "\t// Index des blocs et des threads\n",
        "\tint bx = blockIdx.x;\n",
        "\tint by = blockIdx.y;\n",
        "\tint tx = threadIdx.x;\n",
        "\tint ty = threadIdx.y;\n",
        "    //Variable permettant de stocker la valeur du produit matricielle entre As et Bs\n",
        "    float Csub = 0;\n",
        " \n",
        "\t//On itere sur le nombre de sous-matrices de A et B\n",
        "\tfor (int i = 0; i < NUM_SUBS; i++){\n",
        "\t\t//2.1: Calculer les indices globaux des threads des matrices A et B requis pour faire la copie depuis la memoire globale vers la memoire partagee. \n",
        "        int a_x = ;\n",
        "\t\tint a_y = ;\n",
        "\t\tint b_x = ;\n",
        "\t\tint b_y = ;\n",
        "        \n",
        "        //2.2: Chaque thread doit charger un seul element de A et B dans les sous_matrices As et Bs\n",
        "        As[A remplir][A remplir] =\n",
        "\t\tBs[A remplir][A remplir] =\n",
        "\n",
        "        // Synchronisation pour attendre la fin du chargement des elements par les threads \n",
        "\t\t__syncthreads();\n",
        "        \n",
        "        //2.3: Produit matricielle de la As et Bs\n",
        "\t\tfor (int k = 0; k < BLOCK_SIZE; ++k)\n",
        "\t\t{\n",
        "\t\t\t\n",
        "\t\t}\n",
        "        \n",
        "        // Synchronisation pour attendre la fin du produit matricielle entre les deux sous-matrices As et Bs\n",
        "\t\t__syncthreads();\n",
        "        \n",
        "\t}\n",
        "\n",
        "    //2.4: Calculer les indices globaux des threads de la matrice C\n",
        "\tint c_x = ;\n",
        "\tint c_y = ;\n",
        "    \n",
        "\t// On stocke le resultat dans la matrice C\n",
        "\td_C[c_y][c_x] = Csub;\n",
        "}\n",
        "\n",
        "\n",
        "int main(int argc, char **argv)\n",
        "\n",
        "{\n",
        "\tunsigned int octets_A, octets_B, octets_C;\n",
        "\tunsigned int x, y, errors;\n",
        "\tint maxActiveBlocks;\n",
        "\n",
        "\tevent_pair timer;\n",
        "\n",
        "\tif (A_WIDTH != B_HEIGHT){\n",
        "\t\tprintf(\"Error: A_HEIGHT and B_WIDTH do not match\\n\");\n",
        "\t}\n",
        "\n",
        "\t// 1.1.1 Recuperer la taille en octets des matrices A, B et C.\n",
        "\toctets_A = \n",
        "\toctets_B =\n",
        "\toctets_C =\n",
        "\n",
        "\t// Initialisation de A\n",
        "\tfor (y = 0; y < A_HEIGHT; y++)\n",
        "\tfor (x = 0; x <A_WIDTH; x++)\n",
        "\t\th_A[y][x] = (float)rand() / RAND_MAX;\n",
        "\t// Initialisation de B\n",
        "\tfor (y = 0; y < B_HEIGHT; y++)\n",
        "\tfor (x = 0; x <B_WIDTH; x++)\n",
        "\t\th_B[y][x] = (float)rand() / RAND_MAX;\n",
        "\n",
        "\t// 1.1.2 Copie de la memoire Host sur le Device\n",
        "\tcudaMemcpyToSymbol(A remplir, A remplir, A remplir); //cudaMemcpyToSymbol(destination, source, taille en octets);\n",
        "\tcudaMemcpyToSymbol(A remplir, A remplir, A remplir); //cudaMemcpyToSymbol(destination, source, taille en octets);\n",
        "\tcheckCUDAError(\"CUDA memcpy\");\n",
        "\n",
        "\t// Setup execution parameters\n",
        "\tdim3 threads(BLOCK_SIZE, BLOCK_SIZE);\n",
        "\tdim3 grid(C_WIDTH / BLOCK_SIZE, C_HEIGHT / BLOCK_SIZE);\n",
        "\t\n",
        "    start_timer(&timer);\n",
        "    matMulKernel << < grid, threads >> >();\n",
        "\t\n",
        "    //2.5 lancement du kernel\n",
        "\t//start_timer(&timer);\n",
        "    //matMulKernelSharedMemory << < grid, threads >> >();\n",
        "  \tcudaDeviceSynchronize();  \n",
        "\tstop_timer(&timer,\"Produit matriciel GPU\"); // Rajouter Shared memory dans la chaine de caract√®res pour l'exo 2.5\n",
        "\tcheckCUDAError(\"CUDA kernel execution\");\n",
        "\n",
        "\t// 1.1.3 Copie du resultat depuis le device vers l'host\n",
        "\tcudaMemcpyFromSymbol(A remplir, A remplir, A remplir); // cudaMemcpyFromSymbol(destination, source, taille en octets);\n",
        "\tcheckCUDAError(\"CUDA memcpy results\");\n",
        "\n",
        "\t// Version de la multiplication matricielle sur CPU\n",
        "\tstart_timer(&timer);\n",
        "\tmatMulCPU(h_A, h_B, h_C_ref);\n",
        "\tstop_timer(&timer,\"Produit matriciel GPU\");\n",
        "\t// Check les erreurs\n",
        "\terrors = matMulValidate(h_C, h_C_ref);\n",
        "\tif (errors)\n",
        "\t\tprintf(\"%d Nombre total d'erreur\\n\", errors);\n",
        "\telse\n",
        "\t\tprintf(\"La validation est un succes !\\n\");\n",
        "\n",
        "}\n",
        "\n",
        "\n",
        "void matMulCPU(float A[A_HEIGHT][A_WIDTH], float B[C_HEIGHT][C_WIDTH], float C[C_HEIGHT][C_WIDTH])\n",
        "{\n",
        "\tint x, y, k;\n",
        "\tfor (y = 0; y < C_HEIGHT; y++){\n",
        "\t\tfor (x = 0; x < C_WIDTH; x++){\n",
        "\t\t\tC[y][x] = 0;\n",
        "\t\t\tfor (k = 0; k < A_WIDTH; k++){\n",
        "\t\t\t\tC[y][x] += A[y][k] * B[k][x];\n",
        "\t\t\t}\n",
        "\t\t}\n",
        "\t}\n",
        "\n",
        "}\n",
        "\n",
        "int matMulValidate(float C[C_HEIGHT][C_WIDTH], float Cref[C_HEIGHT][C_WIDTH])\n",
        "{\n",
        "\tint errors = 0;\n",
        "\tint y, x;\n",
        "\tfloat epsilon = 0.001;\n",
        "\tfor (y = 0; y < C_HEIGHT; y++){\n",
        "\t\tfor (x = 0; x < C_WIDTH; x++){\n",
        "\t\t\tif (!(C[y][x] >= Cref[y][x] - epsilon && C[y][x] <= Cref[y][x] + epsilon)){\n",
        "\t\t\t\terrors++;\n",
        "\t\t\t\tprintf(\"Device item c[%d][%d] = %f does not mach host result %f\\n\", y, x, C[y][x], Cref[y][x]);\n",
        "\t\t\t}\n",
        "\t\t}\n",
        "\t}\n",
        "\n",
        "\treturn errors;\n",
        "}\n",
        "\n",
        "void checkCUDAError(const char *msg)\n",
        "{\n",
        "\tcudaError_t err = cudaGetLastError();\n",
        "\tif (cudaSuccess != err)\n",
        "\t{\n",
        "\t\tfprintf(stderr, \"CUDA ERROR: %s: %s.\\n\", msg, cudaGetErrorString(err));\n",
        "\t\texit(EXIT_FAILURE);\n",
        "\t}\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "FbaG8P7hrtJh"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LPQOuQf3sf8D"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "CUDA-GPU.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
